# %%writefile app.py
import os, json, sys, hashlib
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models, transforms

from PIL import Image
import streamlit as st
import pandas as pd

# =========================
# ê¸°ë³¸ ê²½ë¡œ/ì „ì²˜ë¦¬ ì„¤ì •
# =========================
# - ì¼ë°˜ ì‹¤í–‰: __file__ ê¸°ì¤€
# - Colab/Jupyter: __file__ ë¯¸ì¡´ì¬ â†’ os.getcwd() ê¸°ì¤€
try:
    BASE_DIR = Path(__file__).resolve().parent
except NameError:
    BASE_DIR = Path(os.getcwd()).resolve()

ARTIFACTS_DIR = BASE_DIR / "artifacts_3cls"
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

MODEL_PATH = ARTIFACTS_DIR / "resnet18_3cls_best.pth"   # êµ¬ê¸€ë“œë¼ì´ë¸Œì—ì„œ ë°›ì•„ì˜´
MAP_PATH   = ARTIFACTS_DIR / "class_to_idx.json"        # ë ˆí¬ì— í¬í•¨(ê¶Œì¥) ë˜ëŠ” ë“œë¼ì´ë¸Œì—ì„œ ë°›ê¸°

IMG_EXTS = {".png", ".jpg", ".jpeg", ".bmp", ".webp", ".tif", ".tiff", ".gif", ".jfif"}
MEAN = (0.485, 0.456, 0.406)
STD  = (0.229, 0.224, 0.225)
INFER_TF = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(MEAN, STD),
])

def _get_secret_or_env(key: str, default: str = "") -> str:
    # st.secrets ìš°ì„ , ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ default
    try:
        if key in st.secrets:
            return str(st.secrets[key]).strip()
    except Exception:
        pass
    return os.getenv(key, default).strip()

def _sha256sum(p: Path) -> str:
    h = hashlib.sha256()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

# =========================
# ëª¨ë¸/ë¼ë²¨ ì¤€ë¹„ (ë‹¤ìš´ë¡œë“œ + ë¡œë“œ)
# =========================
def ensure_model_download():
    """
    ëª¨ë¸(.pth) íŒŒì¼ì´ ì—†ìœ¼ë©´ Google Driveì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    - FILE_IDëŠ” st.secrets['MODEL_FILE_ID'] ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ MODEL_FILE_ID ë¡œ ì£¼ì… ê¶Œì¥
    - ë“œë¼ì´ë¸Œ ê³µìœ  ì„¤ì •: 'ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì ë³´ê¸°/ë‹¤ìš´ë¡œë“œ'
    - (ì„ íƒ) MODEL_SHA256 ìœ¼ë¡œ ë¬´ê²°ì„± ê²€ì¦ ê°€ëŠ¥
    """
    if MODEL_PATH.exists() and MODEL_PATH.stat().st_size > 0:
        return

    MODEL_FILE_ID = _get_secret_or_env("MODEL_FILE_ID", "")
    if not MODEL_FILE_ID:
        st.error(
            "ëª¨ë¸(.pth) íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
            f"- ê¸°ëŒ€ ê²½ë¡œ: {MODEL_PATH}\n"
            "- Google Drive FILE_IDë¥¼ í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Secretsì— ì„¤ì •í•˜ì„¸ìš”.\n"
            "  Â· í™˜ê²½ë³€ìˆ˜: MODEL_FILE_ID\n"
            "  Â· Streamlit secrets: MODEL_FILE_ID"
        )
        st.stop()

    try:
        import gdown
    except ImportError:
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        import gdown

    url = f"https://drive.google.com/uc?id={MODEL_FILE_ID}&export=download"
    tmp_path = MODEL_PATH.with_suffix(".downloading")

    with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (Google Drive â†’ gdown)"):
        gdown.download(url, str(tmp_path), quiet=False)

    if not tmp_path.exists() or tmp_path.stat().st_size == 0:
        st.error("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ê³¼ FILE_IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    # (ì„ íƒ) ë¬´ê²°ì„± ê²€ì¦
    expected_hash = _get_secret_or_env("MODEL_SHA256", "")
    if expected_hash:
        got = _sha256sum(tmp_path)
        if got.lower() != expected_hash.lower():
            tmp_path.unlink(missing_ok=True)
            st.error("ëª¨ë¸ í•´ì‹œ ë¶ˆì¼ì¹˜: ì—…ë¡œë“œ íŒŒì¼/FILE_ID ë˜ëŠ” SHA256 ê°’ì„ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()

    tmp_path.rename(MODEL_PATH)

def ensure_label_map():
    """
    class_to_idx.json í™•ë³´.
    1) ë ˆí¬/ì´ë¯¸ì§€ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê¶Œì¥: artifacts_3cls/class_to_idx.json ì»¤ë°‹)
    2) ì—†ìœ¼ë©´ Google Driveì—ì„œ ë°›ì•„ì˜¤ê¸° (MAP_FILE_ID í•„ìš”)
    """
    if MAP_PATH.exists() and MAP_PATH.stat().st_size > 0:
        return

    MAP_FILE_ID = _get_secret_or_env("MAP_FILE_ID", "")
    if not MAP_FILE_ID:
        st.error(
            "`class_to_idx.json`ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
            f"- ê¸°ëŒ€ ê²½ë¡œ: {MAP_PATH}\n"
            "- íŒŒì¼ì„ ë¦¬í¬ì— í¬í•¨ì‹œí‚¤ê±°ë‚˜, Google Drive FILE_IDë¥¼ secrets/envë¡œ ì„¤ì •í•˜ì„¸ìš”.\n"
            "  Â· í™˜ê²½ë³€ìˆ˜: MAP_FILE_ID (ì˜µì…˜)\n"
            "  Â· Streamlit secrets: MAP_FILE_ID (ì˜µì…˜)"
        )
        st.stop()

    try:
        import gdown
    except ImportError:
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        import gdown

    url = f"https://drive.google.com/uc?id={MAP_FILE_ID}&export=download"
    with st.spinner("ë¼ë²¨ ë§µ ë‹¤ìš´ë¡œë“œ ì¤‘... (gdown)"):
        gdown.download(url, str(MAP_PATH), quiet=False)

    if not MAP_PATH.exists() or MAP_PATH.stat().st_size == 0:
        st.error("ë¼ë²¨ ë§µ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ê³µìœ  ì„¤ì •ê³¼ MAP_FILE_IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

@st.cache_resource
def load_model_and_labels(arch: str = "resnet18"):
    """ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ í´ë˜ìŠ¤ ë§¤í•‘ì„ ë¡œë“œí•˜ê³ , (model, idx_to_class, class_to_idx)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    ensure_model_download()
    ensure_label_map()

    # ë¼ë²¨ ë¡œë“œ
    try:
        with open(MAP_PATH, "r", encoding="utf-8") as f:
            class_to_idx = json.load(f)
    except Exception as e:
        st.error(f"class_to_idx.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

    # idx->class ë§¤í•‘
    idx_to_class = {v: k for k, v in class_to_idx.items()}
    num_classes = len(idx_to_class)

    # ë°±ë³¸ êµ¬ì„± (í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤)
    if arch == "resnet18":
        backbone = models.resnet18(weights=None)
    elif arch == "resnet50":
        backbone = models.resnet50(weights=None)
    else:
        raise ValueError("Unsupported arch (resnet18|resnet50)")

    in_features = backbone.fc.in_features

    # âš ï¸ í•™ìŠµ ì‹œ MLP í—¤ë“œë¥¼ ì¼ë‹¤ë©´ ë™ì¼ êµ¬ì¡°ë¡œ ë§ì¶°ì•¼ í•¨.
    # í˜„ì¬ëŠ” ë‹¨ì¼ Linearë¡œ ê°€ì • (ì €ì¥ëœ state_dictì™€ ì¼ì¹˜í•´ì•¼ í•¨)
    backbone.fc = nn.Linear(in_features, num_classes)

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    try:
        state = torch.load(MODEL_PATH, map_location="cpu")

        # í˜•íƒœ íŒë³„: ìˆœìˆ˜ state_dict or {"state_dict": ...} or whole model
        if isinstance(state, dict) and "state_dict" in state and isinstance(state["state_dict"], dict):
            backbone.load_state_dict(state["state_dict"], strict=True)
        elif isinstance(state, dict) and all(
            isinstance(k, str) and (
                k.startswith(("conv", "bn", "layer", "fc")) or "num_batches_tracked" in k
            ) for k in state.keys()
        ):
            backbone.load_state_dict(state, strict=True)
        else:
            # ì „ì²´ ëª¨ë¸ ì €ì¥ë³¸ì¼ ê°€ëŠ¥ì„±
            try:
                backbone = state
                if hasattr(backbone, "eval"):
                    backbone.eval()
                else:
                    raise RuntimeError("ë¶ˆì§€ì› í˜•ì‹: ì „ì²´ ëª¨ë¸ ê°ì²´ê°€ ì•„ë‹˜")
            except Exception as e:
                raise RuntimeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì €ì¥ í˜•ì‹ì…ë‹ˆë‹¤: {e}")

    except Exception as e:
        st.error(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

    backbone.eval()
    return backbone, idx_to_class, class_to_idx

def predict_one(img: Image.Image, model: torch.nn.Module, device, idx_to_class: dict):
    x = INFER_TF(img).unsqueeze(0).to(device)
    with torch.no_grad():
        logits = model(x)
        prob = F.softmax(logits, dim=1)[0].detach().cpu().numpy()
    items = [(idx_to_class[i], float(prob[i])) for i in range(len(prob))]
    items.sort(key=lambda t: t[1], reverse=True)
    return items, prob

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Knife/Awl/Scissor Classifier", page_icon="ğŸ”", layout="centered")
st.title("ğŸ” 3-Class Classifier (ResNet)")
st.caption("knife / awl / scissor â€” í™•ë¥  ì˜ˆì¸¡ ë°ëª¨")

# ---- ë””ë²„ê·¸ íŒ¨ë„ (í•„ìš”ì‹œ ì‚¬ìš©) ----
with st.sidebar.expander("ğŸ” Debug (secrets/env)", expanded=False):
    try:
        has_secret = "MODEL_FILE_ID" in st.secrets
    except Exception:
        has_secret = False
    st.write("has st.secrets['MODEL_FILE_ID']:", has_secret)
    st.write("env MODEL_FILE_ID set:", bool(os.getenv("MODEL_FILE_ID")))
    st.write("MODEL_PATH exists:", MODEL_PATH.exists())
    if MODEL_PATH.exists():
        st.write("MODEL_PATH size:", MODEL_PATH.stat().st_size)

# ---- (ì˜µì…˜) ì„ì‹œ ì…ë ¥: Secrets ì—†ì´ í…ŒìŠ¤íŠ¸í•  ë•Œ ì‚¬ìš© í›„ ì‚­ì œ ê¶Œì¥ ----
if _get_secret_or_env("MODEL_FILE_ID", "") == "":
    with st.sidebar.expander("âš ï¸ Set MODEL_FILE_ID (temp)", expanded=False):
        tmp_id = st.text_input("Google Drive FILE_ID")
        if tmp_id:
            os.environ["MODEL_FILE_ID"] = tmp_id
            st.success("MODEL_FILE_ID set for this session. Rerun the app (âŒ˜/Ctrl+R).")

with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, idx_to_class, class_to_idx = load_model_and_labels(arch="resnet18")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

tab1, tab2 = st.tabs(["ğŸ“„ ë‹¨ì¼ ì´ë¯¸ì§€", "ğŸ“ ì—¬ëŸ¬ ì´ë¯¸ì§€(ë°°ì¹˜)"])

with tab1:
    up = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=[ext.lstrip(".") for ext in IMG_EXTS])
    conf_th = st.slider("í‘œì‹œ ì„ê³„ê°’(threshold)", 0.0, 1.0, 0.0, 0.01)
    if up is not None:
        try:
            img = Image.open(up).convert("RGB")
            st.image(img, caption="ì…ë ¥ ì´ë¯¸ì§€", use_column_width=True)
            items, prob = predict_one(img, model, device, idx_to_class)

            df = pd.DataFrame([{"class": c, "prob": p} for c, p in items])
            st.subheader("í™•ë¥ ")
            st.bar_chart(df.set_index("class"))

            top1 = items[0]
            st.markdown(f"**Top-1:** `{top1[0]}` â€” **{top1[1]*100:.2f}%**")
            label = top1[0] if top1[1] >= conf_th else "unknown"
            st.markdown(f"**ìµœì¢… ë¼ë²¨(ì„ê³„ê°’ {conf_th:.2f}):** `{label}`")
        except Exception as e:
            st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

with tab2:
    ups = st.file_uploader("ì—¬ëŸ¬ ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=[ext.lstrip(".") for ext in IMG_EXTS], accept_multiple_files=True)
    if ups:
        rows = []
        for f in ups:
            try:
                img = Image.open(f).convert("RGB")
                items, prob = predict_one(img, model, device, idx_to_class)
                top1 = items[0]
                row = {"filename": f.name, "pred": top1[0], "conf": top1[1]}
                # ê³ ì • ìˆœì„œ: class_to_idxì˜ ì¸ë±ìŠ¤ ìˆœ
                for cls in sorted(class_to_idx, key=lambda k: class_to_idx[k]):
                    idx = class_to_idx[cls]
                    row[f"p_{cls}"] = float(prob[idx])
                rows.append(row)
            except Exception as e:
                rows.append({"filename": f.name, "pred": "ERROR", "conf": 0.0, "error": str(e)})

        if rows:
            out_df = pd.DataFrame(rows)
            st.dataframe(out_df, use_container_width=True)
            st.download_button(
                "CSV ë‹¤ìš´ë¡œë“œ",
                data=out_df.to_csv(index=False).encode("utf-8"),
                file_name="infer_results.csv",
                mime="text/csv",
            )
